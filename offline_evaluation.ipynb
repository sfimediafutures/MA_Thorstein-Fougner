{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882a253e",
      "metadata": {
        "id": "882a253e"
      },
      "outputs": [],
      "source": [
        "!pip install implicit tqdm --quiet\n",
        "\n",
        "import os\n",
        "import random\n",
        "import threadpoolctl\n",
        "\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "threadpoolctl.threadpool_limits(1, \"blas\")\n",
        "\n",
        "import time\n",
        "import heapq\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from typing import List, Optional, Any\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import csr_matrix, coo_matrix\n",
        "from tabulate import tabulate\n",
        "from tqdm import tqdm\n",
        "from gcsfs import GCSFileSystem\n",
        "from pandas_gbq import to_gbq\n",
        "from google.auth import exceptions\n",
        "from google.cloud import bigquery\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "from implicit.nearest_neighbours import bm25_weight\n",
        "from implicit.evaluation import train_test_split, precision_at_k, ranking_metrics_at_k"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9rCImI4uRt6",
      "metadata": {
        "id": "d9rCImI4uRt6"
      },
      "source": [
        "# Evaluation metrics & methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jf2QGnt9uUfg",
      "metadata": {
        "id": "Jf2QGnt9uUfg"
      },
      "source": [
        "### Precision:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ck15MFnmMD1w",
      "metadata": {
        "id": "Ck15MFnmMD1w"
      },
      "outputs": [],
      "source": [
        "def precision(recommended_items, relevant_items):\n",
        "    if len(recommended_items) == 0:\n",
        "        return 0\n",
        "    relevant_in_rec = sum(1 for item in recommended_items if item in relevant_items)\n",
        "    precision = relevant_in_rec / len(recommended_items)\n",
        "    return precision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zGX9CuQBua7E",
      "metadata": {
        "id": "zGX9CuQBua7E"
      },
      "source": [
        "### Mean Average Precision at K (MAP@K):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FjzQU2i0BBjX",
      "metadata": {
        "id": "FjzQU2i0BBjX"
      },
      "outputs": [],
      "source": [
        "def calculate_average_precision(recommended_items, relevant_items):\n",
        "    ap = 0.0\n",
        "    num_relevant = 0\n",
        "\n",
        "    for i, item in enumerate(recommended_items):\n",
        "        if item in relevant_items:\n",
        "            num_relevant += 1\n",
        "            ap += num_relevant / (i + 1)\n",
        "\n",
        "    if num_relevant > 0:\n",
        "        ap /= num_relevant\n",
        "    return ap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd_VFMhhGHFy",
      "metadata": {
        "id": "bd_VFMhhGHFy"
      },
      "source": [
        "### Gini coefficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fz5VlxJhb1i",
      "metadata": {
        "id": "0fz5VlxJhb1i"
      },
      "outputs": [],
      "source": [
        "def gini(array: dict):\n",
        "    array = np.array(list(array.values()))\n",
        "    if array.size == 0:\n",
        "        return 0.0\n",
        "    if np.amin(array) < 0:\n",
        "        array -= np.amin(array)\n",
        "\n",
        "    array = np.sort(array)\n",
        "    index = np.arange(1, array.shape[0] + 1)\n",
        "    n = array.shape[0]\n",
        "    return ((np.sum((2 * index - n - 1) * array)) / (n * np.sum(array)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tAP-AOUZ7lAg",
      "metadata": {
        "id": "tAP-AOUZ7lAg"
      },
      "source": [
        "# Offline Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d47773",
      "metadata": {
        "id": "c9d47773"
      },
      "source": [
        "### BigQuery globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sV4dbhtD7rKf",
      "metadata": {
        "id": "sV4dbhtD7rKf"
      },
      "outputs": [],
      "source": [
        "RUN_ID = \"als-3month-datapoints-context\"  # @param {type:\"string\"}\n",
        "MODEL = \"None\"  # @param {type:\"string\"}\n",
        "\n",
        "BUCKET_NAME = \"\"  # @param {type:\"string\"}\n",
        "DATA_PATH = \"\"  # @param {type:\"string\"}\n",
        "DATA_URI = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Local specs\n",
        "TO_LOCAL = False  # @param {type:\"boolean\"}\n",
        "FILENAME = None  # @param {type:\"string\"}\n",
        "\n",
        "# Bigquery specs\n",
        "TO_BIGQUERY = True  # @param {type:\"boolean\"}\n",
        "BQ_DATASET_ID = \"\"  # @param {type:\"string\"}\n",
        "BQ_TABLE_ID = \"\"  # @param {type:\"string\"}\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AhiEY2Z8FHYU",
      "metadata": {
        "id": "AhiEY2Z8FHYU"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G6tx5OZ_7sHD",
      "metadata": {
        "id": "G6tx5OZ_7sHD"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wAGtVNWL7tFG",
      "metadata": {
        "id": "wAGtVNWL7tFG"
      },
      "outputs": [],
      "source": [
        "def load_csv_from_gcs(data_uri, n_items=1):\n",
        "    # List of all Parquet files in the directory\n",
        "\n",
        "    gcs = GCSFileSystem()\n",
        "\n",
        "    # Get all files in the bucket path\n",
        "    files = gcs.glob(f\"{data_uri}/**\")  # `**` ensures recursive search\n",
        "    # Filter for Parquet files\n",
        "    files = [f\"gs://{file}\" for file in files if file.endswith(\".csv\")]\n",
        "    # Combine all Parquet files into a single DataFrame\n",
        "    df = pd.concat([pd.read_csv(file) for file in files[:n_items]], ignore_index=True)\n",
        "    print(\"Rows in DF before preprocessing:\", df.shape[0])\n",
        "    df = df[df[\"contentType\"] == \"SERIES\"]\n",
        "    df.rename(columns={'durationSec': 'playingTime'}, inplace=True)\n",
        "    df.loc[:, 'firstStart'] = pd.to_datetime(df['firstStart'], errors='coerce')\n",
        "    df = df[df['playingTime'] >= 250]\n",
        "    df['playingTime'] = df['playingTime'].clip(upper=12000)\n",
        "    user_item_counts = df.groupby('profileId')['itemId'].count()\n",
        "    valid_users = user_item_counts[user_item_counts >= 5].index\n",
        "    df = df[df['profileId'].isin(valid_users)]\n",
        "    print(\"Rows in DF after preprocessing:\", df.shape[0])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dy6KFAAAS579",
      "metadata": {
        "id": "Dy6KFAAAS579"
      },
      "source": [
        "### Splitting / preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gj0Y31nCAuiN",
      "metadata": {
        "id": "Gj0Y31nCAuiN"
      },
      "outputs": [],
      "source": [
        "def prepare_data(dataframe, apply_bm25=True, K1=3.0, B=1.5, split_date='2024-10-22'):\n",
        "    print(\"Rows in DF:\", dataframe.shape[0])\n",
        "\n",
        "    # Split into train and test using date\n",
        "    print(\"Splitting\")\n",
        "    split_date = pd.to_datetime(split_date)\n",
        "    train_dataframe = dataframe[dataframe['firstStart'] < split_date].copy()\n",
        "    test_dataframe = dataframe[dataframe['firstStart'] >= split_date].copy()\n",
        "\n",
        "    # Force the types as string -> integers\n",
        "    print(\"Converting to category\")\n",
        "    train_dataframe['profileId'] = train_dataframe['profileId'].astype(str).astype('category')\n",
        "    train_dataframe['itemId'] = train_dataframe['itemId'].astype(str).astype('category')\n",
        "\n",
        "    test_dataframe['profileId'] = test_dataframe['profileId'].astype(str).astype('category')\n",
        "    test_dataframe['itemId'] = test_dataframe['itemId'].astype(str).astype('category')\n",
        "\n",
        "    print(\"Creating sparse matrix\")\n",
        "    items_mapping = dict(enumerate(train_dataframe['itemId'].cat.categories))\n",
        "    users_mapping = dict(enumerate(train_dataframe['profileId'].cat.categories))\n",
        "\n",
        "    train_matrix = coo_matrix(\n",
        "        (train_dataframe['playingTime'].astype(np.float32),\n",
        "         (train_dataframe['profileId'].cat.codes,\n",
        "          train_dataframe['itemId'].cat.codes))\n",
        "    ).tocsr()\n",
        "\n",
        "    # BM25 transformation\n",
        "    print(\"Applying BM25\")\n",
        "    if apply_bm25:\n",
        "        train_matrix = bm25_weight(train_matrix, K1=K1, B=B).tocsr()\n",
        "\n",
        "    # Reverse mappings\n",
        "    reverse_user_mapping = {idx: profile_id for profile_id, idx in users_mapping.items()}\n",
        "    reverse_item_mapping = {idx: item_id for item_id, idx in items_mapping.items()}\n",
        "\n",
        "    # If a user is not present in the user_mapping, e.g. the user does not\n",
        "    # appear in the training set, we will get a NaN\n",
        "    test_dataframe.loc[:, 'user_index'] = test_dataframe['profileId'].map(reverse_user_mapping)\n",
        "    test_dataframe.loc[:, 'item_index'] = test_dataframe['itemId'].map(reverse_item_mapping)\n",
        "\n",
        "    nan_user_indices = test_dataframe['user_index'].isna().sum()\n",
        "    nan_user_indices_item = test_dataframe['item_index'].isna().sum()\n",
        "    print(f\"sum of missing users in test_dataframe: {nan_user_indices}\")\n",
        "    print(f\"sum of missing items in test_dataframe: {nan_user_indices_item}\")\n",
        "\n",
        "    # Drop the cases where the item or user has not been seen in the test cases\n",
        "    len_before_removing_nan = len(test_dataframe)\n",
        "    test_dataframe = test_dataframe.dropna(subset=['user_index', 'item_index'])\n",
        "    print(f\"Removing rows where user_index or item_index is NaN from test_dataframe: from {len_before_removing_nan} to {len(test_dataframe)}, removed {len_before_removing_nan - len(test_dataframe)}\")\n",
        "\n",
        "    # Convert indices to integers\n",
        "    test_dataframe['user_index'] = test_dataframe['user_index'].astype(int)\n",
        "    test_dataframe['item_index'] = test_dataframe['item_index'].astype(int)\n",
        "\n",
        "    print(\"Create duration scores\")\n",
        "    duration_scores = train_dataframe.groupby('itemId', observed=False)['playingTime'].sum().to_dict()\n",
        "\n",
        "    len_before_removing_user_index = len(test_dataframe)\n",
        "    test_dataframe = test_dataframe[test_dataframe['user_index'].isin(train_matrix.indptr)]\n",
        "    print(f\"Removing rows where user_index not in train_matrix: from {len_before_removing_user_index} to {len(test_dataframe)}, removed {len_before_removing_user_index - len(test_dataframe)}\")\n",
        "\n",
        "    # Group by 'user_index' and take the first 5 rows for each group, we're only intrested in this.\n",
        "    len_before_5_cap = len(test_dataframe)\n",
        "    test_dataframe = test_dataframe.groupby('user_index').head(5)\n",
        "    print(f\"Grouping and limiting test to 5 users, from {len_before_5_cap} to {len(test_dataframe)}, removed {len_before_5_cap - len(test_dataframe)}\")\n",
        "\n",
        "    print(f\"Test rows after removing missing users: {len(test_dataframe)}\")\n",
        "    test_cases = np.array(\n",
        "        list(zip(test_dataframe['user_index'].astype(int), test_dataframe['itemId'].astype(str), test_dataframe['firstStart']))\n",
        "    )\n",
        "\n",
        "    print(\"\\nSIZE:\")\n",
        "    print(f\"Train: {len(train_dataframe) / len(dataframe) * 100:.2f}%\")\n",
        "    print(f\"Test: {len(test_dataframe) / len(dataframe) * 100:.2f}%\")\n",
        "    print(f\"Test Cases: {len(test_cases)}\")\n",
        "    print(\"\\nSHAPE:\")\n",
        "    print(f\"Sparse matrix (train): {train_matrix.shape}\")\n",
        "\n",
        "    print(\"\\nUSERS & ITEMS\")\n",
        "    print(f\"Min playingTime: {dataframe['playingTime'].min()}, Max playingTime: {dataframe['playingTime'].max()}\")\n",
        "    print(f\"Users in df: {dataframe['profileId'].nunique()}, Items in df: {dataframe['itemId'].nunique()}\")\n",
        "    print(f\"Users in mapping: {len(users_mapping)}, Items in mapping: {len(items_mapping)}\")\n",
        "\n",
        "    return train_matrix, users_mapping, items_mapping, reverse_user_mapping, test_cases, duration_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NGaOHf9q4NqT",
      "metadata": {
        "id": "NGaOHf9q4NqT"
      },
      "outputs": [],
      "source": [
        "df = load_csv_from_gcs(DATA_URI, n_items=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FaE5ooCzTDxP",
      "metadata": {
        "id": "FaE5ooCzTDxP"
      },
      "outputs": [],
      "source": [
        "train_matrix, users_mapping, items_mapping, reverse_user_mapping, test_cases, duration_scores = prepare_data(\n",
        "    df, apply_bm25=False, K1=3.0, B=1.5, split_date='2024-10-14')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d0d328a",
      "metadata": {
        "id": "6d0d328a"
      },
      "source": [
        "### Applying time of day context to test cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v-cdpefPw_Il",
      "metadata": {
        "id": "v-cdpefPw_Il"
      },
      "outputs": [],
      "source": [
        "# Convert to time of day context\n",
        "test_cases = [\n",
        "    (user_id, str(itemid), [\"night\", \"morning\",\n",
        "     \"afternoon\", \"evening\"][(context.hour // 6) % 4])\n",
        "    for user_id, itemid, context in test_cases\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bINSE7mlFnzW",
      "metadata": {
        "id": "bINSE7mlFnzW"
      },
      "outputs": [],
      "source": [
        "test_cases[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19fe3dcc",
      "metadata": {
        "id": "19fe3dcc"
      },
      "source": [
        "### Top K most popular items for popularity baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stdZMpSk5m1c",
      "metadata": {
        "id": "stdZMpSk5m1c"
      },
      "outputs": [],
      "source": [
        "# Ranking items by popularity\n",
        "item_counts = Counter(df['itemId'])\n",
        "items_ranked_by_popularity = [str(item) for item, _ in item_counts.most_common()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V8L3njZ39bq3",
      "metadata": {
        "id": "V8L3njZ39bq3"
      },
      "outputs": [],
      "source": [
        "del df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28D-ObV1C4d6",
      "metadata": {
        "id": "28D-ObV1C4d6"
      },
      "source": [
        "### Get contextual weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "in7QAAu5uCNZ",
      "metadata": {
        "id": "in7QAAu5uCNZ"
      },
      "outputs": [],
      "source": [
        "client = bigquery.Client()\n",
        "\n",
        "query = \"\"\"\n",
        "WITH ranked_items AS (\n",
        "    SELECT *,\n",
        "           ROW_NUMBER() OVER(PARTITION BY itemId ORDER BY date DESC) as rn\n",
        "    FROM `mytable`\n",
        ")\n",
        "SELECT *\n",
        "FROM ranked_items\n",
        "WHERE rn = 1;\n",
        "\"\"\"\n",
        "\n",
        "tod_context_scores = client.query(query).to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PB3gPmCQD7cD",
      "metadata": {
        "id": "PB3gPmCQD7cD"
      },
      "outputs": [],
      "source": [
        "# Hashmap of all context scores for quick lookup\n",
        "context_mapping = {\n",
        "    \"timeofday\": tod_context_scores.set_index('itemId').to_dict(orient='index'),\n",
        "}\n",
        "\n",
        "del tod_context_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c97e4ee",
      "metadata": {
        "id": "1c97e4ee"
      },
      "source": [
        "## Post-filtering methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uo5aYaZYHVZx",
      "metadata": {
        "id": "uo5aYaZYHVZx"
      },
      "outputs": [],
      "source": [
        "def rerank_with_context(context_rec_item_ids: List[int], context_rec_item_scores: List[int], context_type, context, n, w=None):\n",
        "    missing_context_item = 0\n",
        "    contextualized_scores = []\n",
        "    context_mapping_type = context_mapping.get(context_type, {})\n",
        "    for item_idx, score in zip(context_rec_item_ids, context_rec_item_scores):\n",
        "        item_id = items_mapping.get(item_idx)\n",
        "        context_score = context_mapping.get(\n",
        "            context_type, {}).get(item_id, {}).get(context, 1)\n",
        "        if context_score == 1:\n",
        "            missing_context_item += 1\n",
        "            continue\n",
        "        post_filtering_score = context_score * score\n",
        "        # string, int\n",
        "        contextualized_scores.append((item_id, post_filtering_score))\n",
        "\n",
        "    top_n_scores = heapq.nlargest(n, contextualized_scores, key=lambda x: x[1])\n",
        "    return top_n_scores, missing_context_item"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RhLgxP7QybA1",
      "metadata": {
        "id": "RhLgxP7QybA1"
      },
      "source": [
        "## Evaluation methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qeXpEaspkujK",
      "metadata": {
        "id": "qeXpEaspkujK"
      },
      "outputs": [],
      "source": [
        "def combine_metrics(coverage_map, hitrate_map, popularity_list, map_score, precision_list, gini_index_map):\n",
        "    avg_coverage = len(coverage_map) / train_matrix.shape[0]\n",
        "    avg_hitrate = np.mean(hitrate_map)\n",
        "    avg_popularity = sum(popularity_list) / len(popularity_list) if popularity_list else 0\n",
        "    avg_map = map_score / len(test_cases)\n",
        "    avg_precision = np.mean(precision_list)\n",
        "    gini_value = gini(gini_index_map)\n",
        "    return avg_coverage, avg_hitrate, avg_popularity, avg_map, avg_precision, gini_value\n",
        "\n",
        "\n",
        "def compile_results(combination_id, factors, reg, iterations, k, coverage, hitrate, popularity, gini, map_score, precision, missing_ids, missing_context_item, model_name, avg_filtered_for_user, train_time, custom_scoring_time, full_model_run_time, start_time, test_cases):\n",
        "    return {\n",
        "        'id': combination_id,\n",
        "        'factors': int(factors),\n",
        "        'regularization': float(reg),\n",
        "        'iterations': int(iterations),\n",
        "        'k': int(k),\n",
        "        'coverage': float(coverage),\n",
        "        'hitrate': float(hitrate),\n",
        "        'popularity': float(popularity),\n",
        "        'gini': float(gini),\n",
        "        'map': float(map_score),\n",
        "        'precision': float(precision),\n",
        "        'missing_id_in_test': int(missing_ids),\n",
        "        'missing_context_item': int(missing_context_item),\n",
        "        'model': model_name,\n",
        "        'run_id': f'{RUN_ID}',\n",
        "        'filtered_from_observed': int(avg_filtered_for_user),\n",
        "        'train_time': float(train_time),\n",
        "        'custom_scoring_time': float(custom_scoring_time),\n",
        "        'full_model_eval_time': float(full_model_run_time),\n",
        "        'started': float(start_time),\n",
        "        'ended': float(time.time()),\n",
        "        'train_matrix_shape': str(train_matrix.shape),\n",
        "        'test_cases': str(len(test_cases))\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluation(model, train_matrix, k, n_context, combination_id, factors, reg, iterations, train_time, start_time):\n",
        "    als_coverage_map, context_coverage_map = set(), set()\n",
        "    random_coverage_map, popularity_coverage_map = set(), set()\n",
        "\n",
        "    random_gini_index_map = {item_id: 0 for item_id in items_ranked_by_popularity}\n",
        "    popularity_gini_index_map = {item_id: 0 for item_id in items_ranked_by_popularity}\n",
        "\n",
        "    als_gini_index_map = {item_id: 0 for item_id in items_ranked_by_popularity}\n",
        "    context_gini_index_map = {item_id: 0 for item_id in items_ranked_by_popularity}\n",
        "\n",
        "    als_popularity, context_popularity = [], []\n",
        "    random_popularity, popularity_popularity = [], []\n",
        "\n",
        "    als_hitrate_map, context_hitrate_map = [], []\n",
        "    random_hitrate_map, popularity_hitrate_map = [], []\n",
        "\n",
        "    als_map_score, context_map_score = 0.0, 0.0\n",
        "    random_map_score, popularity_map_score = 0.0, 0.0\n",
        "\n",
        "    als_precision, context_precision = [], []\n",
        "    random_precision, popularity_precision = [], []\n",
        "\n",
        "    filtered_for_user = [0, 0, 0]\n",
        "    missing_context_item = 0\n",
        "    missing_ids = 0\n",
        "    start_custom_scoring_time = time.time()\n",
        "\n",
        "    for user_idx, itemId, context in test_cases:\n",
        "        rec_item_ids, rec_item_scores = model.recommend(userid=user_idx, user_items=train_matrix[user_idx], N=n_context)\n",
        "\n",
        "        # ALS\n",
        "        als_rec_item_ids = rec_item_ids[:k]\n",
        "        als_coverage_map.update(als_rec_item_ids)\n",
        "\n",
        "        rec_item_ids_dfid = [items_mapping.get(item_idx) for item_idx in als_rec_item_ids]\n",
        "        if len(rec_item_ids_dfid) == 0:\n",
        "            raise ValueError(\"Error, rec_item_ids_dfid is empty, something is wrong with the mappings\")\n",
        "\n",
        "        als_map_score += calculate_average_precision(rec_item_ids_dfid, [itemId])\n",
        "        als_precision.append(precision(rec_item_ids_dfid, [itemId]))\n",
        "        als_hitrate_map.append(int(any(item in rec_item_ids_dfid for item in [itemId])))\n",
        "\n",
        "        for item_idx in rec_item_ids_dfid:\n",
        "            als_gini_index_map[item_idx] = als_gini_index_map.get(item_idx, 0) + 1\n",
        "            duration_score = duration_scores.get(item_idx, None)\n",
        "            if duration_score:\n",
        "                als_popularity.append(duration_score)\n",
        "\n",
        "        # Context\n",
        "        context_rec_tof_sorted, missing_context_item_count = rerank_with_context(rec_item_ids, rec_item_scores, 'timeofday', context, n=k)\n",
        "        missing_context_item += missing_context_item_count\n",
        "\n",
        "        context_item_ids = [item[0] for item in context_rec_tof_sorted]\n",
        "        context_coverage_map.update(context_item_ids)\n",
        "\n",
        "        context_map_score += calculate_average_precision(context_item_ids, [itemId])\n",
        "        context_precision.append(precision(context_item_ids, [itemId]))\n",
        "        context_hitrate_map.append(int(any(item in [itemId] for item in context_item_ids)))\n",
        "\n",
        "        for item_idx in context_item_ids:\n",
        "            context_gini_index_map[item_idx] = context_gini_index_map.get(item_idx, 0) + 1\n",
        "            duration_score = duration_scores.get(item_idx)\n",
        "            if duration_score:\n",
        "                context_popularity.append(duration_score)\n",
        "\n",
        "        # Random\n",
        "        random_item_ids = np.random.choice(items_ranked_by_popularity, k, replace=False)\n",
        "        random_coverage_map.update(random_item_ids)\n",
        "\n",
        "        random_map_score += calculate_average_precision(random_item_ids, [itemId])\n",
        "        random_precision.append(precision(random_item_ids, [itemId]))\n",
        "        random_hitrate_map.append(int(any(item in random_item_ids for item in [itemId])))\n",
        "\n",
        "        for item_idx in random_item_ids:\n",
        "            random_gini_index_map[item_idx] = random_gini_index_map.get(item_idx, 0) + 1\n",
        "            duration_score = duration_scores.get(item_idx, None)\n",
        "            if duration_score:\n",
        "                random_popularity.append(duration_score)\n",
        "\n",
        "        # Popularity\n",
        "        popularity_item_ids = items_ranked_by_popularity[:k]\n",
        "        popularity_coverage_map.update(popularity_item_ids)\n",
        "\n",
        "        popularity_map_score += calculate_average_precision(popularity_item_ids, [itemId])\n",
        "        popularity_precision.append(precision(popularity_item_ids, [itemId]))\n",
        "        popularity_hitrate_map.append(int(any(item in popularity_item_ids for item in [itemId])))\n",
        "\n",
        "        for item_idx in popularity_item_ids:\n",
        "            popularity_gini_index_map[item_idx] = popularity_gini_index_map.get(item_idx, 0) + 1\n",
        "            duration_score = duration_scores.get(item_idx, None)\n",
        "            if duration_score:\n",
        "                popularity_popularity.append(duration_score)\n",
        "\n",
        "    # Aggregate metrics\n",
        "    als_avg_coverage, als_avg_hitrate, als_avg_popularity, als_avg_map, als_avg_precision, als_gini_value = combine_metrics(\n",
        "        als_coverage_map, als_hitrate_map, als_popularity, als_map_score, als_precision, als_gini_index_map)\n",
        "\n",
        "    context_avg_coverage, context_avg_hitrate, context_avg_popularity, context_avg_map, context_avg_precision, context_gini_value = combine_metrics(\n",
        "        context_coverage_map, context_hitrate_map, context_popularity, context_map_score, context_precision, context_gini_index_map)\n",
        "\n",
        "    random_avg_coverage, random_avg_hitrate, random_avg_popularity, random_avg_map, random_avg_precision, random_gini_value = combine_metrics(\n",
        "        random_coverage_map, random_hitrate_map, random_popularity, random_map_score, random_precision, random_gini_index_map)\n",
        "\n",
        "    popularity_avg_coverage, popularity_avg_hitrate, popularity_avg_popularity, popularity_avg_map, popularity_avg_precision, popularity_gini_value = combine_metrics(\n",
        "        popularity_coverage_map, popularity_hitrate_map, popularity_popularity, popularity_map_score, popularity_precision, popularity_gini_index_map)\n",
        "\n",
        "    end_custom_scoring_time = time.time() - start_custom_scoring_time\n",
        "    full_model_run_time = time.time() - start_time\n",
        "\n",
        "    avg_filtered_for_user = np.mean(np.array(filtered_for_user))\n",
        "\n",
        "    # Create results\n",
        "    als_results = compile_results(\n",
        "        combination_id, factors, reg, iterations, k, als_avg_coverage, als_avg_hitrate, als_avg_popularity, als_gini_value,\n",
        "        als_avg_map, als_avg_precision, missing_ids, 0, f'{MODEL}', avg_filtered_for_user, train_time, end_custom_scoring_time,\n",
        "        full_model_run_time, start_time, test_cases\n",
        "    )\n",
        "\n",
        "    context_results = compile_results(\n",
        "        combination_id, factors, reg, iterations, k, context_avg_coverage, context_avg_hitrate, context_avg_popularity,\n",
        "        context_gini_value, context_avg_map, context_avg_precision, missing_ids, missing_context_item, f'{MODEL}_contextualized',\n",
        "        avg_filtered_for_user, train_time, end_custom_scoring_time, full_model_run_time, start_time, test_cases\n",
        "    )\n",
        "\n",
        "    random_results = compile_results(\n",
        "        combination_id, factors, reg, iterations, k, random_avg_coverage, random_avg_hitrate, random_avg_popularity,\n",
        "        random_gini_value, random_avg_map, random_avg_precision, missing_ids, 0, f'{MODEL}_random', avg_filtered_for_user,\n",
        "        train_time, end_custom_scoring_time, full_model_run_time, start_time, test_cases\n",
        "    )\n",
        "\n",
        "    popularity_results = compile_results(\n",
        "        combination_id, factors, reg, iterations, k, popularity_avg_coverage, popularity_avg_hitrate, popularity_avg_popularity,\n",
        "        popularity_gini_value, popularity_avg_map, popularity_avg_precision, missing_ids, 0, f'{MODEL}_popularity', avg_filtered_for_user,\n",
        "        train_time, end_custom_scoring_time, full_model_run_time, start_time, test_cases\n",
        "    )\n",
        "\n",
        "    return als_results, context_results, random_results, popularity_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZSs6MDWzLAVl",
      "metadata": {
        "id": "ZSs6MDWzLAVl"
      },
      "source": [
        "## Grid search evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p6_ilz_lK-5N",
      "metadata": {
        "id": "p6_ilz_lK-5N"
      },
      "outputs": [],
      "source": [
        "MODEL = \"ALS\"\n",
        "\n",
        "param_grid = {\n",
        "    'factors': [20, 50, 75, 100],\n",
        "    'regularization': [0.01, 0.1, 0.5],\n",
        "    'iterations': [10, 30, 50],\n",
        "    'k': [1, 3, 10, 20],\n",
        "}\n",
        "\n",
        "total_rounds = len(param_grid['factors']) * len(param_grid['regularization']\n",
        "                                                ) * len(param_grid['iterations']) * len(param_grid['k'])\n",
        "print(f\"Will run {total_rounds} times.\")\n",
        "\n",
        "results = []\n",
        "combinations = list(itertools.product(param_grid['factors'],\n",
        "                                      param_grid['regularization'],\n",
        "                                      param_grid['iterations'],\n",
        "                                      param_grid['k']))\n",
        "\n",
        "\n",
        "i = 0\n",
        "with tqdm(total=total_rounds) as pbar:\n",
        "    for combination in combinations:\n",
        "        results = []\n",
        "        combination_id = i\n",
        "        factors, reg, iterations, k = combination\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            model = AlternatingLeastSquares(factors=factors,\n",
        "                                            regularization=reg,\n",
        "                                            iterations=iterations,\n",
        "                                            calculate_training_loss=True,\n",
        "                                            use_native=True,\n",
        "                                            use_cg=True,\n",
        "                                            num_threads=0,\n",
        "                                            random_state=42)\n",
        "\n",
        "            model.fit(train_matrix, show_progress=False)\n",
        "            train_time = time.time() - start_time\n",
        "\n",
        "            n_context = 100\n",
        "            als_results, context_results, random_results, popularity_results = evaluation(model, train_matrix, k, n_context, combination_id, factors, reg, iterations, train_time, start_time)\n",
        "\n",
        "            results.append(als_results)\n",
        "            results.append(context_results)\n",
        "            results.append(random_results)\n",
        "            results.append(popularity_results)\n",
        "\n",
        "            if TO_BIGQUERY:\n",
        "                result_df = pd.DataFrame(results)\n",
        "                to_gbq(result_df, f'{BQ_DATASET_ID}.{BQ_TABLE_ID}', project_id=PROJECT_ID, if_exists='append')\n",
        "\n",
        "        except (exceptions.GoogleAuthError, exceptions.TransportError) as e:\n",
        "            print(f\"Error uploading to BigQuery: {e}, combination_id: {combination_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during processing: {e}, combination_id: {combination_id}\")\n",
        "\n",
        "        pbar.update(1)\n",
        "        i += 1\n",
        "\n",
        "if TO_LOCAL:\n",
        "    als_results = pd.DataFrame(results)\n",
        "    als_results.to_csv('offline_eval_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_va7EBtuzVWf",
      "metadata": {
        "id": "_va7EBtuzVWf"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r5dB5YnTzX_7",
      "metadata": {
        "id": "r5dB5YnTzX_7"
      },
      "outputs": [],
      "source": [
        "client = bigquery.Client()\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT * FROM mytable\n",
        "WHERE run_id = \"{RUN_ID}\"\n",
        "\"\"\"\n",
        "\n",
        "df = client.query(query).to_dataframe()\n",
        "\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "offline_evaluation_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rec-kernel",
      "language": "python",
      "name": "rec"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
